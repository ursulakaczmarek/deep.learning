{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### intro to gradient descent\n",
    "fully connected neural networks are a sequence of matrices that map input vectors to output vectors by multiplying the matrices (layers) by intermediate vectors (activations). fitting these matrix products requires that we adjust the weights, or values of the matrices, so that when the input vectors pass through the matrices, we get predicted output vectors close to the true output vectors. \n",
    "\n",
    "we start with random weight values and use an optimization algoritm to get close to target output vector *y*. we initialize the matrix weights either manually (such as Xavier: var(weights) = 2/n<sub>in</sub> + n<sub>out</sub>), or with a deep learning library. the optimization algorithm first discussed in the fast.ai deep learning course is gradient descent. there are two flavors of gradient descent: standard and stochastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math, sys, os, numpy as np\n",
    "from numpy.random import random\n",
    "from matplotlib import pyplot as plt, rcParams, animation, rc\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from ipywidgets.widgets import *\n",
    "rc('animation', html = 'html5')\n",
    "rcParams['figure.figsize'] = 3, 3\n",
    "%precision 4\n",
    "np.set_printoptions(precision = 4, linewidth = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stochastic gradient descent iteratively selects parameters (weights) to reduce the loss function, that is, the method of calculating the difference between the predicted outputs and the true outputs. to illustrate, we start with sum of squared errors as the loss function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start with a line of unknown parameters\n",
    "def lin(a, b, x): return a*x+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create parameters to start\n",
    "a = 3\n",
    "b = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomly generated weights as matrix elements\n",
    "# a powerful concept: starting with completely random weights yet finding a solution through iteration\n",
    "n = 30\n",
    "x = random(n)\n",
    "y = lin(a, b, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define our loss function: sum of squared errors\n",
    "def sse(y, y_pred): return ((y-y_pred)**2).sum()\n",
    "def loss(y, a, b, x): return sse(y, lin(a, b, x))\n",
    "def avg_loss(y, a, b, x): return np.sqrt(loss(y, a, b, x)/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to minimize this averaged loss function value. our linear regression function has two \n",
    "numerical inputs: the *x* elements that represents the matrix values, and the *a*, *b* parameters we\n",
    "specified above. we cannot change the matrix values, so we want our gradient descent optimizer to \n",
    "choose parameters that minimize the loss function value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the average loss is high if our guesses are bad, low if our guesses are good\n",
    "a_guess = -1.\n",
    "b_guess = 1.\n",
    "avg_loss(y, a_guess, b_guess, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradients are vectors that represent how the loss function changes with respect to each parameter. \n",
    "gradient descent entails iteratively calculating the partial derivative of the loss function with respect to each of our parameters and updating the parameters in the direction opposite that derivative. if the derivative of *a* is positive, increasing *a* increases the loss function. we get a positive gradient, so we want to move in the opposite direction by decreasing *a*. likewise, if the derivative is negative, the loss function decreases with each increase in *a* so we want to continue increasing *a*. in both cases, we want to move in the direction opposite the derivative.\n",
    "\n",
    "gradient descent updates parameter values until it reaches parameter values that can no longer decrease the loss function value. this is the local minimum. \n",
    "\n",
    "the size of each of the algorithm's updates is the learning rate. high learning rates cover more ground more quickly, but have a higher risk of overshooting the local minimum. with lower learning rates, the algorithm more frequently takes steps (looks for negative gradients), thus increasing precision but at the expense of speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define our learning rate as a little step \n",
    "# generally want the highest number we can get away with\n",
    "lr = 0.01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define our function for finding the optimal local minimum\n",
    "# del[(y-(a*x+b))**2, b] = 2(b +a*x -y) = 2(y_pred - y)\n",
    "# del[(y-(a*x+b))**2,a] = 2 x (b + a x - y)    = x * dy/db \n",
    "def update_weights(a, b, x, y, lr):\n",
    "    global a_guess, b_guess\n",
    "    y_pred = lin(a_guess, b_guess, x)\n",
    "    dydb = 2 * (y_pred - y) # as b increases one unit, sse changes by 2 * (y_pred - y)\n",
    "    dyda = x * dydb # as a increases one unit, sse changes by x * dydb\n",
    "    a_guess -= lr * dyda.mean()\n",
    "    b_guess -= lr * dydb.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# animate the change in sse loss function\n",
    "fig = plt.figure(dpi = 100, figsize = (5, 4))\n",
    "plt.scatter(x, y)\n",
    "line, = plt.plot(x, lin(a_guess, b_guess, x))\n",
    "plt.close()\n",
    "\n",
    "def animate(i):\n",
    "    line.set_ydata(lin(a_guess, b_guess, x))\n",
    "    for i in range(10): \n",
    "        update_weights()\n",
    "    return line\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, np.arange(0, 40), interval = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ani"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
